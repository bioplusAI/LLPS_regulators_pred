{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1066308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pathlib\n",
    "import torch\n",
    "import esm\n",
    "from esm import pretrained\n",
    "from esm import FastaBatchedDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403bc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(output_dir, fasta_file, tokens_per_batch=4096, seq_length=7096, repr_layers=[36]):\n",
    "    model, alphabet = pretrained.esm2_t36_3B_UR50D()\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    filenames = []  \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in tqdm(enumerate(data_loader), total=len(batches)):\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                filenames.append(filename)  \n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id}\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "\n",
    "                torch.save(result, filename)\n",
    "    return filenames  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f70249-62af-47c9-abf8-0aff5696be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Start from the current directory\n",
    "current_path = pathlib.Path().resolve()\n",
    "\n",
    "\n",
    "# Traverse upwards until we find 'LLPS_regulators_pred'\n",
    "while current_path.name != \"LLPS_regulators_pred\":\n",
    "    if current_path.parent == current_path:\n",
    "        raise FileNotFoundError(\"Project root 'LLPS_regulators_pred' not found in path hierarchy.\")\n",
    "    current_path = current_path.parent\n",
    "\n",
    "project_root = current_path\n",
    "fasta_file = project_root / \"Input_sequences\" / \"Input_sequences.txt\"\n",
    "output_dir = project_root / \"embeddings\"\n",
    "\n",
    "# Run the function\n",
    "extract_embeddings(output_dir, fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e87e74e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\AppData\\Local\\anaconda3\\envs\\bib\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024E08A48F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024E08A495A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Regulator in LLPS\n",
      "Non-Regulator in LLPS\n"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "import torch\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# Load the query sequence representations\n",
    "def load_protein_representations(folder_path, files):\n",
    "    queryproteinrep = []\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            rep_changes = torch.load(file_path)['mean_representations'][36]\n",
    "            queryproteinrep.append(rep_changes.tolist())\n",
    "        else:\n",
    "            print(f\"File {file_path} not found.\")\n",
    "    return torch.tensor(queryproteinrep)\n",
    "\n",
    "\n",
    "\n",
    "# Automatically detect project root\n",
    "current_path = pathlib.Path().resolve()\n",
    "while current_path.name != \"LLPS_regulators_pred\":\n",
    "    if current_path.parent == current_path:\n",
    "        raise FileNotFoundError(\"Project root 'LLPS_regulators_pred' not found in path hierarchy.\")\n",
    "    current_path = current_path.parent\n",
    "\n",
    "project_root = current_path\n",
    "\n",
    "# Path to sequence representations\n",
    "folder_path = project_root / \"embeddings\"\n",
    "files_test = sorted(os.listdir(folder_path))\n",
    "query_rep = load_protein_representations(folder_path, files_test)\n",
    "query_rep = query_rep.numpy()\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(2560,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  \n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "\n",
    "# Load all saved models for ensemble predictions\n",
    "import glob\n",
    "\n",
    "# Path to models directory using project_root\n",
    "models_path = project_root / \"models\"\n",
    "model_files = sorted(models_path.glob(\"dataset_*.h5\"))\n",
    "\n",
    "# Convert Path objects to strings for compatibility if needed\n",
    "#model_files = [str(file) for file in model_files]\n",
    "\n",
    "# Initialize lists for ensemble predictions\n",
    "loaded_predictions = []\n",
    "loaded_probabilities = []\n",
    "\n",
    "\n",
    "for model_file in model_files:\n",
    "    loaded_model = load_model(model_file, compile = False)\n",
    "    # Make predictions on the test set\n",
    "    y_pred_proba = loaded_model.predict(query_rep, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    loaded_predictions.append(y_pred)\n",
    "    loaded_probabilities.append(y_pred_proba)\n",
    "    \n",
    "# Convert predictions and probabilities to numpy arrays\n",
    "ensemble_predictions = np.array(loaded_predictions)  \n",
    "ensemble_probabilities = np.array(loaded_probabilities) \n",
    "\n",
    "# Using the model to make predictions\n",
    "predictions = model.predict(query_rep)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# After all folds are processed, calculate the final ensemble accuracy and other metrics\n",
    "ensemble_predictions = np.array(ensemble_predictions)  \n",
    "ensemble_probabilities = np.array(ensemble_probabilities) \n",
    "\n",
    "# Majority voting for final predictions\n",
    "votes = np.sum(ensemble_predictions, axis=0)  \n",
    "majority_decision = (votes > (ensemble_predictions.shape[0] // 2)).astype(int)\n",
    "\n",
    "# Handle ties (if any)\n",
    "ties = (votes == ensemble_predictions.shape[0] // 2)  \n",
    "if np.any(ties):\n",
    "    avg_probabilities = ensemble_probabilities.mean(axis=0)\n",
    "    majority_decision[ties] = (avg_probabilities[ties] >= 0.5).astype(int)\n",
    "\n",
    "# Final ensemble predictions\n",
    "final_predictions = majority_decision\n",
    "final_probabilities = ensemble_probabilities.mean(axis=0)\n",
    "\n",
    "for final_prediction in final_predictions:\n",
    "    if final_prediction == 1:\n",
    "        print(\"Regulator in LLPS\")\n",
    "    if final_prediction == 0:\n",
    "        print(\"Non-Regulator in LLPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81218a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
